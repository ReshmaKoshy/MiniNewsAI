{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae822136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 01_DATA_PREPARATION.IPYNB\n",
    "# ============================================================================\n",
    "# PURPOSE: Load, explore, and split dataset into train/val/test sets\n",
    "# TIME ESTIMATE: 10-15 minutes (CPU only, no GPU needed)\n",
    "# ============================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d85cc4f",
   "metadata": {},
   "source": [
    "# Multiclass Classification - Data Preparation (Stage 2 Input)\n",
    "\n",
    "## Objective\n",
    "Prepare the news article dataset for multiclass classification (SAFE, SENSITIVE, UNSAFE):\n",
    "- Create stratified train/val/test splits\n",
    "- Output files will be used as input for Stage 2 multiclass classifier training\n",
    "\n",
    "## Strategy\n",
    "- Use `summary_long_500` (400 words) as input text\n",
    "- 70/15/15 stratified split by label AND category\n",
    "- Save train/val/test datasets as stage2_input files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3410d6",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e44801e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from collections import Counter\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Configure display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "notebook_dir = os.getcwd()\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(notebook_dir, '..'))\n",
    "\n",
    "# Create results directory for this notebook\n",
    "RESULTS_DIR = os.path.join(PROJECT_ROOT, 'results', 'data_preparation')\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Results directory: {RESULTS_DIR}\")\n",
    "print(\"✓ Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9259d24e",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Dataset\n",
    "\n",
    "**Rationale:** Understanding data distribution is critical for:\n",
    "- Identifying class imbalance (UNSAFE is minority class at ~19.7%)\n",
    "- Ensuring all categories are represented\n",
    "- Detecting any data quality issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202e13c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "DATA_PATH = os.path.join(PROJECT_ROOT, 'data', 'raw', 'stage2_multiclass_classification_input.csv')\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DATASET OVERVIEW\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nShape: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "print(f\"\\nData types:\\n{df.dtypes}\")\n",
    "print(f\"\\nMissing values:\\n{df.isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a0b9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample rows\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAMPLE DATA\")\n",
    "print(\"=\"*80)\n",
    "print(df[['category', 'label', 'summary_long_500']].head(3))\n",
    "\n",
    "# Check text lengths\n",
    "df['text_length_words'] = df['summary_long_500'].str.split().str.len()\n",
    "df['text_length_chars'] = df['summary_long_500'].str.len()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEXT LENGTH STATISTICS (summary_long_500)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Average words: {df['text_length_words'].mean():.0f}\")\n",
    "print(f\"Min words: {df['text_length_words'].min()}\")\n",
    "print(f\"Max words: {df['text_length_words'].max()}\")\n",
    "print(f\"Std words: {df['text_length_words'].std():.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015666cc",
   "metadata": {},
   "source": [
    "## 3. Analyze Label Distribution\n",
    "\n",
    "**Key Insight:** UNSAFE is the minority class (~19.7%)\n",
    "- This imbalance requires special handling (class weights, focal loss)\n",
    "- Our goal: 90%+ UNSAFE recall (catch 90%+ of dangerous content)\n",
    "- Trade-off: Accept lower precision (more false alarms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b50b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LABEL DISTRIBUTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "label_counts = df['label'].value_counts()\n",
    "label_percentages = df['label'].value_counts(normalize=True) * 100\n",
    "\n",
    "label_stats = pd.DataFrame({\n",
    "    'Count': label_counts,\n",
    "    'Percentage': label_percentages\n",
    "})\n",
    "print(label_stats)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Count plot\n",
    "axes[0].bar(label_counts.index, label_counts.values, color=['green', 'orange', 'red'])\n",
    "axes[0].set_title('Label Distribution (Counts)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_xlabel('Label')\n",
    "for i, v in enumerate(label_counts.values):\n",
    "    axes[0].text(i, v + 50, str(v), ha='center', fontweight='bold')\n",
    "\n",
    "# Percentage plot\n",
    "axes[1].bar(label_percentages.index, label_percentages.values, color=['green', 'orange', 'red'])\n",
    "axes[1].set_title('Label Distribution (Percentage)', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('Percentage (%)')\n",
    "axes[1].set_xlabel('Label')\n",
    "for i, v in enumerate(label_percentages.values):\n",
    "    axes[1].text(i, v + 1, f'{v:.1f}%', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(os.path.join(RESULTS_DIR, 'label_distribution.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"✓ Label distribution saved to: {os.path.join(RESULTS_DIR, 'label_distribution.png')}\")\n",
    "\n",
    "# Calculate imbalance ratio\n",
    "unsafe_count = label_counts['UNSAFE']\n",
    "safe_count = label_counts['SAFE']\n",
    "imbalance_ratio = safe_count / unsafe_count\n",
    "print(f\"\\nImbalance ratio (SAFE:UNSAFE): {imbalance_ratio:.2f}:1\")\n",
    "print(\"→ This is MODERATE imbalance (not extreme)\")\n",
    "print(\"→ Weighted loss + focal loss should handle this well\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7582e44d",
   "metadata": {},
   "source": [
    "## 4. Analyze Category Distribution\n",
    "\n",
    "**Rationale:** Ensure stratified split maintains category balance\n",
    "- Different categories may have different UNSAFE patterns\n",
    "- Want representative samples from each category in train/val/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358c2dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CATEGORY DISTRIBUTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "category_counts = df['category'].value_counts()\n",
    "print(category_counts)\n",
    "\n",
    "# Cross-tabulation: Label x Category\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LABEL × CATEGORY CROSS-TABULATION\")\n",
    "print(\"=\"*80)\n",
    "cross_tab = pd.crosstab(df['category'], df['label'], margins=True)\n",
    "print(cross_tab)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 6))\n",
    "cross_tab_pct = pd.crosstab(df['category'], df['label'], normalize='index') * 100\n",
    "cross_tab_pct.plot(kind='bar', stacked=False, color=['green', 'orange', 'red'])\n",
    "plt.title('Label Distribution by Category', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Percentage (%)')\n",
    "plt.xlabel('Category')\n",
    "plt.legend(title='Label')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(RESULTS_DIR, 'category_label_distribution.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"✓ Category-label distribution saved to: {os.path.join(RESULTS_DIR, 'category_label_distribution.png')}\")\n",
    "\n",
    "print(\"\\n→ Categories have varying UNSAFE percentages\")\n",
    "print(\"→ ENVIRONMENT has highest sample count (3256)\")\n",
    "print(\"→ SCIENCE has lowest sample count (1760)\")\n",
    "print(\"→ Stratified split will maintain these proportions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4e82d5",
   "metadata": {},
   "source": [
    "## 5. Prepare Data for Splitting\n",
    "\n",
    "**Strategy:** \n",
    "- Select only needed columns: text, label, category\n",
    "- Create combined stratification key (label_category)\n",
    "- This ensures splits maintain both label AND category proportions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9316df06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select and rename columns\n",
    "df_clean = df[['summary_long_500', 'label', 'category']].copy()\n",
    "df_clean.columns = ['text', 'label', 'category']\n",
    "\n",
    "# Remove temporary length columns from original df\n",
    "df = df.drop(['text_length_words', 'text_length_chars'], axis=1, errors='ignore')\n",
    "\n",
    "print(f\"\\nCleaned dataset shape: {df_clean.shape}\")\n",
    "print(f\"Columns: {df_clean.columns.tolist()}\")\n",
    "\n",
    "# Create stratification column\n",
    "# This ensures splits maintain proportions across BOTH label AND category\n",
    "df_clean['strat_key'] = df_clean['label'] + '_' + df_clean['category']\n",
    "\n",
    "print(f\"\\nUnique stratification keys: {df_clean['strat_key'].nunique()}\")\n",
    "print(\"(Should be 12: 3 labels × 4 categories)\")\n",
    "\n",
    "# Verify no missing values\n",
    "assert df_clean.isnull().sum().sum() == 0, \"ERROR: Missing values detected!\"\n",
    "print(\"\\n✓ No missing values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca61be0d",
   "metadata": {},
   "source": [
    "## 6. Stratified Train/Val/Test Split\n",
    "\n",
    "**Split Strategy:**\n",
    "- 70% Train (6,925 samples) - For model training\n",
    "- 15% Validation (1,484 samples) - For hyperparameter tuning, threshold selection\n",
    "- 15% Test (1,484 samples) - For final evaluation (NEVER touch until end)\n",
    "\n",
    "**Why Stratified?**\n",
    "- Maintains ~19.7% UNSAFE in all splits\n",
    "- Maintains category proportions in all splits\n",
    "- Prevents train/val/test from having different distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed15d5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PERFORMING STRATIFIED SPLIT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# First split: 70% train, 30% temporary\n",
    "train_df, temp_df = train_test_split(\n",
    "    df_clean,\n",
    "    test_size=0.30,\n",
    "    random_state=RANDOM_SEED,\n",
    "    stratify=df_clean['strat_key']  # Stratify by label + category\n",
    ")\n",
    "\n",
    "# Second split: Split temp into 50-50 (giving us 15% val, 15% test)\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df,\n",
    "    test_size=0.50,\n",
    "    random_state=RANDOM_SEED,\n",
    "    stratify=temp_df['strat_key']\n",
    ")\n",
    "\n",
    "# Remove stratification key (not needed anymore)\n",
    "train_df = train_df.drop('strat_key', axis=1).reset_index(drop=True)\n",
    "val_df = val_df.drop('strat_key', axis=1).reset_index(drop=True)\n",
    "test_df = test_df.drop('strat_key', axis=1).reset_index(drop=True)\n",
    "\n",
    "print(f\"Train size: {len(train_df)} ({len(train_df)/len(df_clean)*100:.1f}%)\")\n",
    "print(f\"Val size: {len(val_df)} ({len(val_df)/len(df_clean)*100:.1f}%)\")\n",
    "print(f\"Test size: {len(test_df)} ({len(test_df)/len(df_clean)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc49e00",
   "metadata": {},
   "source": [
    "## 7. Verify Split Quality\n",
    "\n",
    "**Critical Check:** Ensure UNSAFE percentage is consistent across splits\n",
    "- If splits have very different UNSAFE%, the split failed\n",
    "- All should be ~19.7% ± 1%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16a8596",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SPLIT VERIFICATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def analyze_split(df, split_name):\n",
    "    \"\"\"Analyze label and category distribution in a split\"\"\"\n",
    "    print(f\"\\n{split_name.upper()} SET:\")\n",
    "    print(f\"  Total samples: {len(df)}\")\n",
    "    \n",
    "    # Label distribution\n",
    "    label_dist = df['label'].value_counts()\n",
    "    label_pct = df['label'].value_counts(normalize=True) * 100\n",
    "    \n",
    "    print(f\"\\n  Label distribution:\")\n",
    "    for label in ['SAFE', 'SENSITIVE', 'UNSAFE']:\n",
    "        count = label_dist.get(label, 0)\n",
    "        pct = label_pct.get(label, 0)\n",
    "        print(f\"    {label}: {count} ({pct:.1f}%)\")\n",
    "    \n",
    "    # Category distribution\n",
    "    category_dist = df['category'].value_counts()\n",
    "    print(f\"\\n  Category distribution:\")\n",
    "    for cat in category_dist.index:\n",
    "        print(f\"    {cat}: {category_dist[cat]}\")\n",
    "    \n",
    "    return label_pct.get('UNSAFE', 0)\n",
    "\n",
    "# Analyze each split\n",
    "train_unsafe_pct = analyze_split(train_df, 'train')\n",
    "val_unsafe_pct = analyze_split(val_df, 'validation')\n",
    "test_unsafe_pct = analyze_split(test_df, 'test')\n",
    "\n",
    "# Check consistency\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"UNSAFE PERCENTAGE CONSISTENCY CHECK\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Train: {train_unsafe_pct:.2f}%\")\n",
    "print(f\"Val:   {val_unsafe_pct:.2f}%\")\n",
    "print(f\"Test:  {test_unsafe_pct:.2f}%\")\n",
    "print(f\"Difference: {max(train_unsafe_pct, val_unsafe_pct, test_unsafe_pct) - min(train_unsafe_pct, val_unsafe_pct, test_unsafe_pct):.2f}%\")\n",
    "\n",
    "if abs(train_unsafe_pct - val_unsafe_pct) < 1.5 and abs(train_unsafe_pct - test_unsafe_pct) < 1.5:\n",
    "    print(\"\\n✓ SPLIT QUALITY: EXCELLENT\")\n",
    "    print(\"  All splits have similar UNSAFE percentages\")\n",
    "else:\n",
    "    print(\"\\n⚠ SPLIT QUALITY: CHECK NEEDED\")\n",
    "    print(\"  Splits have different UNSAFE percentages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f9129e",
   "metadata": {},
   "source": [
    "## 8. Ready to Save Datasets\n",
    "\n",
    "**Next Step:** Save train/val/test splits as Stage 2 input files for multiclass classifier training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67071c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PREPARING STAGE 2 INPUT DATASETS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nDatasets will be saved as:\")\n",
    "print(\"  - train_stage2_input.csv\")\n",
    "print(\"  - val_stage2_input.csv\")\n",
    "print(\"  - test_stage2_input.csv\")\n",
    "print(\"\\nThese files contain 3-class labels (SAFE, SENSITIVE, UNSAFE)\")\n",
    "print(\"and will be used for multiclass classifier training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e56300e",
   "metadata": {},
   "source": [
    "## 9. Calculate Class Weights (Optional)\n",
    "\n",
    "**Purpose:** Compute class weights for handling imbalanced classes during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e78d761",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CALCULATING CLASS WEIGHTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate weights for 3-class classification\n",
    "train_labels = train_df['label'].values\n",
    "label_classes = np.unique(train_labels)\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=label_classes,\n",
    "    y=train_labels\n",
    ")\n",
    "\n",
    "print(\"\\nClass Weights (for multiclass classification):\")\n",
    "class_weight_dict = {}\n",
    "for cls, weight in zip(label_classes, class_weights):\n",
    "    class_weight_dict[cls] = weight\n",
    "    print(f\"  {cls}: {weight:.2f}\")\n",
    "\n",
    "print(\"\\n→ These weights can be used during model training to handle class imbalance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda2971c",
   "metadata": {},
   "source": [
    "## 10. Save Datasets as Stage 2 Input Files\n",
    "\n",
    "**Purpose:** Save train/val/test splits as stage2_input files for multiclass classifier training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b970868d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAVING STAGE 2 INPUT DATASETS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save datasets as stage2_input files\n",
    "train_df.to_csv(os.path.join(PROJECT_ROOT, 'data', 'processed', 'train_stage2_input.csv'), index=False)\n",
    "val_df.to_csv(os.path.join(PROJECT_ROOT, 'data', 'processed', 'val_stage2_input.csv'), index=False)\n",
    "test_df.to_csv(os.path.join(PROJECT_ROOT, 'data', 'processed', 'test_stage2_input.csv'), index=False)\n",
    "print(\"✓ Saved train_stage2_input.csv\")\n",
    "print(\"✓ Saved val_stage2_input.csv\")\n",
    "print(\"✓ Saved test_stage2_input.csv\")\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'total_samples': len(df_clean),\n",
    "    'train_samples': len(train_df),\n",
    "    'val_samples': len(val_df),\n",
    "    'test_samples': len(test_df),\n",
    "    'label_distribution': df_clean['label'].value_counts().to_dict(),\n",
    "    'category_distribution': df_clean['category'].value_counts().to_dict(),\n",
    "    'class_weights': class_weight_dict,\n",
    "    'random_seed': RANDOM_SEED\n",
    "}\n",
    "\n",
    "with open(os.path.join(PROJECT_ROOT, 'data', 'processed', 'metadata.json'), 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "print(\"✓ Saved metadata.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd032eb",
   "metadata": {},
   "source": [
    "## 11. Create Keyword Filter List (Optional)\n",
    "\n",
    "**Purpose:** Catch obvious UNSAFE content without running the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37a1beb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1828818b",
   "metadata": {},
   "source": [
    "## 12. Create Keyword Filter List\n",
    "\n",
    "**Purpose:** Catch obvious UNSAFE content without running the model\n",
    "- Adds 5-7% to UNSAFE recall\n",
    "- Reduces model load\n",
    "- Acts as first line of defense\n",
    "\n",
    "**Strategy:**\n",
    "- Extract high-frequency words from UNSAFE articles\n",
    "- Manually curate known danger signals\n",
    "- Use for preprocessing in inference pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b294ac42",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CREATING UNSAFE KEYWORD FILTER\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Extract top words from UNSAFE articles\n",
    "# unsafe_texts = train_df[train_df['label'] == 'UNSAFE']['text'].values\n",
    "# all_unsafe_text = ' '.join(unsafe_texts).lower()\n",
    "\n",
    "# Simple word frequency count (you can use TF-IDF for better results)\n",
    "# from collections import Counter\n",
    "# import re\n",
    "\n",
    "# # Tokenize and count\n",
    "# words = re.findall(r'\\b[a-z]{4,}\\b', all_unsafe_text)  # Words with 4+ letters\n",
    "# word_freq = Counter(words)\n",
    "\n",
    "# # Get top 50 most common words in UNSAFE articles\n",
    "# top_unsafe_words = [word for word, count in word_freq.most_common(100)]\n",
    "# print(top_unsafe_words)\n",
    "\n",
    "# Manual high-confidence UNSAFE keywords (curated list)\n",
    "manual_keywords = [\n",
    "    'rape', 'raped', 'sexual assault', 'sexually assaulted', 'molest', 'molestation',\n",
    "    'pedophile', 'child abuse', 'abused', 'violence', 'violent', 'murder', 'killed',\n",
    "    'death', 'weapon', 'gun', 'shooting', 'shot', 'blood', 'injury', 'injured',\n",
    "    'assault', 'attacked', 'victim', 'predator', 'harassment', 'harassed',\n",
    "    'explicit', 'pornography', 'nude', 'naked', 'inappropriate', 'misconduct', 'suicide', 'assassination'\n",
    "]\n",
    "\n",
    "# Combine (remove duplicates)\n",
    "#combined_keywords = list(set(manual_keywords))\n",
    "\n",
    "print(f\"Manual high-confidence keywords: {len(manual_keywords)}\")\n",
    "#print(f\"Combined unique keywords: {len(combined_keywords)}\")\n",
    "\n",
    "# Preview\n",
    "print(\"\\nSample keywords:\")\n",
    "print(manual_keywords[:20])\n",
    "\n",
    "# Save to file\n",
    "with open(os.path.join(PROJECT_ROOT, 'data', 'keywords', 'unsafe_keywords.txt'), 'w') as f:\n",
    "    for keyword in sorted(manual_keywords):\n",
    "        f.write(f\"{keyword}\\n\")\n",
    "\n",
    "print(\"\\n✓ Saved unsafe_keywords.txt\")\n",
    "print(\"\\nNOTE: Review and refine this list manually!\")\n",
    "print(\"Add domain-specific terms based on news categories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d4077f",
   "metadata": {},
   "source": [
    "## 12. Summary and Next Steps\n",
    "\n",
    "**What we accomplished:**\n",
    "✓ Loaded and explored news articles from stage2_multiclass_classification_input.csv\n",
    "✓ Created stratified 70/15/15 train/val/test split\n",
    "✓ Saved datasets as stage2_input files (train_stage2_input.csv, val_stage2_input.csv, test_stage2_input.csv)\n",
    "✓ Calculated class weights for imbalance handling\n",
    "✓ Created keyword filter for preprocessing (optional)\n",
    "\n",
    "**Next steps:**\n",
    "1. Train multiclass classifier model → Notebook 02 (02_multiclass_classifier_training.ipynb)\n",
    "2. Train kid-safe rewriter model → Notebook 03 (03_kid_safe_rewriter_training.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdeecd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA PREPARATION COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate actual sizes dynamically\n",
    "train_size = len(train_df)\n",
    "val_size = len(val_df)\n",
    "test_size = len(test_df)\n",
    "\n",
    "print(\"\\nGenerated files:\")\n",
    "print(\"  data/processed/\")\n",
    "print(f\"    ├── train_stage2_input.csv ({train_size:,} samples)\")\n",
    "print(f\"    ├── val_stage2_input.csv ({val_size:,} samples)\")\n",
    "print(f\"    ├── test_stage2_input.csv ({test_size:,} samples)\")\n",
    "print(\"    └── metadata.json\")\n",
    "print(\"  data/keywords/\")\n",
    "print(\"    └── unsafe_keywords.txt (optional)\")\n",
    "\n",
    "# Display split breakdown\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATASET SPLIT SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTotal samples: {len(df_clean):,}\")\n",
    "print(f\"  Train: {train_size:,} ({train_size/len(df_clean)*100:.1f}%)\")\n",
    "print(f\"  Val:   {val_size:,} ({val_size/len(df_clean)*100:.1f}%)\")\n",
    "print(f\"  Test:  {test_size:,} ({test_size/len(df_clean)*100:.1f}%)\")\n",
    "\n",
    "# Label distribution across splits\n",
    "print(\"\\nLabel distribution across splits:\")\n",
    "for split_name, split_df in [('Train', train_df), ('Val', val_df), ('Test', test_df)]:\n",
    "    unsafe_pct = (split_df['label'] == 'UNSAFE').sum() / len(split_df) * 100\n",
    "    sensitive_pct = (split_df['label'] == 'SENSITIVE').sum() / len(split_df) * 100\n",
    "    safe_pct = (split_df['label'] == 'SAFE').sum() / len(split_df) * 100\n",
    "    print(f\"  {split_name:5s}: UNSAFE={unsafe_pct:5.1f}%  SENSITIVE={sensitive_pct:5.1f}%  SAFE={safe_pct:5.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"READY FOR MULTICLASS CLASSIFIER TRAINING!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nNext: Open 02_multiclass_classifier_training.ipynb\")\n",
    "print(\"\\nThese files (train/val/test_stage2_input.csv) will be used as input for Stage 2 multiclass classifier training.\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
